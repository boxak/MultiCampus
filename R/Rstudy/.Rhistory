cat("평균:",mean,"\n")
#factor로 만들었으면 더 쉬웠을듯...
#table은 빈도 수를 집계해주는 함수
a <- table(data2)
class(a)
a
paste0("가장 많이 등장한 단어는 ",keyList[maxInx],"입니다")
fac <- factor(data2)
summary(fac)
# 교재 81페이지
library()
installed.packages()
search()
read_excel()
install.packages("readxl")
library(readxl) # require(readxl)
read_excel("book/data_ex.xls")
getwd()
read_excel("book/data_ex.xls")
search() ## 로드된 패키지 함수들을 보여준다.
excel_data_ex <- read_excel("book/data_ex.xls")
View(excel_data_ex)
View(excel_data_ex)
library(MASS, lib.loc = "C:/Program Files/R/R-3.6.3/library")
detach("package:MASS", unload = TRUE)
install.packages("rvest")
library(rvest)
text <- read_html(url)
library(rvest)
url <- "http://unico2013.dothome.co.kr/crawling/tagstyle.html"
text <- read_html(url)
text
View(text)
View(text)
str(text)
nodes <- html_nodes(text, "div")
nodes
title <- html_text(nodes)
title
node1 <- html_nodes(text, "div:nth-of-type(1)")
node1
node1 <- html_nodes(text, "div:nth-child(1)")
node1
node1 <- html_nodes(text, "div:nth-of-type(1)")
node1
html_text(node1)
html_attr(node1, "style")
node2 <- html_nodes(text, "div:nth-of-type(2)")
node2
html_text(node2)
html_attr(node2, "style")
node3 <- html_nodes(text, "div:nth-of-type(3)")
node3
html_text(node3)
library(rvest)
url <- "http://unico2013.dothome.co.kr/crawling/exercise_bs.html"
webpage <- read_html(url,encoding = "UTF-8")
h1.nodes <- html_nodes(webpage,"h1")
h1.nodes
h1.texts <- html_text(h1.nodes)
a.nodes
a.nodes <- html_nodes(webpage,"a")
a.nodes
a.attrs <- html_attrs(a.nodes)
a.attrs
a.texts <- html_text(a.nodes)
a.texts
h1.nodes
h1.texts
h1.texts <- html_text(h1.nodes,trim = TRUE)
a.texts <- html_text(a.nodes,trim = TRUE)
a.texts
is.na(a.texts)
a.texts[is.na(a.texts) | is.null(a.texts) | a.texts==""]
a.texts[is.na(a.texts) | a.texts==""]
a.texts[is.na(a.texts) | a.texts==""] <- NULL
a.texts
is.na(a.texts) | a.texts==""
a.texts[is.na(a.texts) | a.texts==""] <- NULL
a.texts[!(is.na(a.texts) | a.texts=="")]
a.texts <- a.texts[!(is.na(a.texts) | a.texts=="")]
a.texts
rm(webpage)
setwd("function")
getwd()
source("crawler.R",encoding = "UTF-8")
img.attrs <- page.getAttr(url,encoding,"img")
img.attrs
url <- "http://unico2013.dothome.co.kr/crawling/exercise_bs.html"
encoding = "UTF-8"
img.attrs <- page.getAttr(url,encoding,"img")
img.attrs
View(img.attrs)
css = "h2:nth-of-type(1)"
h2.first.texts <- page.getContent(url,trim = TRUE,css = css)
h2.first.texts
css = 'ul > *[style$="green"]'
ul.green.texts <- page.getContent(url,css = css,trim = TRUE)
ul.green.texts
css = "h2:nth-of-type(2)"
h2.second.texts <- page.getContent(url,css)
css = "h2:nth-of-type(2)"
h2.second.texts <- page.getContent(url,css = css)
h2.second.texts
css = "ul > *"
ul.all.texts <- page.getContent(url,css = css)
ul.all.texts
css = "table *"
table.all.texts <- page.getContent(url,css = css)
table.all.texts
ul.all.texts
css = "tr#name"
tr.name.texts <- page.getContent(url,css = css)
tr.name.texts
css = "tr.name"
tr.name.texts <- page.getContent(url,css = css)
tr.name.texts
View(a.attrs)
css ="td.target"
td.target.texts <- page.getContent(url,css = css)
td.target.texts
css ="td#target"
td.target.texts <- page.getContent(url,css = css)
td.target.texts
img.attrs
table.all.texts
css = "ol > *"
ul.all.texts <- page.getContent(url,css = css)
ul.all.texts
url <- "http://unico2013.dothome.co.kr/crawling/tagstyle.html"
text <- read_html(url) # euc-kr 이면 CP949로 인코딩해야함.
text
str(text)
text<- NULL
url<- "http://movie.naver.com/movie/point/af/list.nhn?page=1"
text <- read_html(url,  encoding="CP949")
text
nodes <- html_nodes(text, ".movie")
title <- html_text(nodes)
title
nodes <- html_nodes(text, ".title em")
point <- html_text(nodes)
point
nodes <- html_nodes(text, xpath='//*[@id="old_content"]/table/tbody/tr[1]/td[2]/text()')
imsi <- html_text(nodes, trim=TRUE)
review <- imsi[nchar(imsi) > 0]
review
if(length(review) == 10) {
page <- cbind(title, point)
page <- cbind(page, review)
write.csv(page, "movie_reviews.csv")
} else {
cat("리뷰글이 생략된 데이터가 있네요ㅜㅜ\n")
}
nodes <- html_nodes(text, ".movie")
title <- html_text(nodes,trim = TRUE)
title
nodes <- html_nodes(text, ".title em")
point <- html_text(nodes)
point
nodes <- html_nodes(text, xpath='//*[@id="old_content"]/table/tbody/tr[1]/td[2]/text()')
imsi <- html_text(nodes, trim=TRUE)
review <- imsi[nchar(imsi) > 0]
review
if(length(review) == 10) {
page <- cbind(title, point)
page <- cbind(page, review)
write.csv(page, "movie_reviews.csv")
} else {
cat("리뷰글이 생략된 데이터가 있네요ㅜㅜ\n")
}
nodes <- html_nodes(text, xpath='//*[@id="old_content"]/table/tbody/tr/td[2]/text()')
imsi <- html_text(nodes, trim=TRUE)
review <- imsi[nchar(imsi) > 0]
review
if(length(review) == 10) {
page <- cbind(title, point)
page <- cbind(page, review)
write.csv(page, "movie_reviews.csv")
} else {
cat("리뷰글이 생략된 데이터가 있네요ㅜㅜ\n")
}
nodes <- html_nodes(text, xpath='//*[@id="old_content"]/table/tbody/tr[1]/td[2]/text()')
imsi <- html_text(nodes, trim=TRUE)
review <- imsi[nchar(imsi) > 0]
review
if(length(review) == 10) {
page <- cbind(title, point)
page <- cbind(page, review)
write.csv(page, "movie_reviews.csv")
} else {
cat("리뷰글이 생략된 데이터가 있네요ㅜㅜ\n")
}
nodes <- html_nodes(text, xpath='//*[@id="old_content"]/table/tbody/tr/td[2]/text()')
imsi <- html_text(nodes, trim=TRUE)
review <- imsi[nchar(imsi) > 0]
review
if(length(review) == 10) {
page <- cbind(title, point)
page <- cbind(page, review)
write.csv(page, "movie_reviews.csv")
} else {
cat("리뷰글이 생략된 데이터가 있네요ㅜㅜ\n")
}
setwd("function")
source("crawler.R",encoding = "UTF-8")
getwd()
url <- "https://movie.daum.net/moviedb/grade?movieId=127122"
css.score <- page.getContent(url,css = "em.emph_grade")
css.review <- page.getContent(url,css = "p.desc_review")
css.score
css.review
DaumMovieReview <- data.frame(
댓글리뷰=css.review,
고객평점=css.score
)
View(DaumMovieReview)
?write.csv
write.csv(DaumMovieReview,file="data/daummovie1.csv",row.names = TRUE,
col.names = TRUE,fileEncoding = "UTF-8")
write.table(DaumMovieReview,file="data/daummovie1.csv",sep=",",row.names = TRUE,
col.names = TRUE,fileEncoding = "UTF-8")
getwd()
write.table(DaumMovieReview,file="./data/daummovie1.csv",sep=",",row.names = TRUE,
col.names = TRUE,fileEncoding = "UTF-8")
write.table(DaumMovieReview,file="C:\\boxak\\Rstudy\\data\\daummovie.csv",sep=",",row.names = TRUE,
col.names = TRUE,fileEncoding = "UTF-8")
write.table(DaumMovieReview,file="C:\\boxak\\Rstudy\\data\\daummovie.csv",sep=",",row.names = TRUE,
col.names = TRUE,fileEncoding = "CP949")
write.table(DaumMovieReview,file="C:\\boxak\\Rstudy\\data\\daummovie.csv",sep=",",row.names = FALSE,
col.names = TRUE,fileEncoding = "CP949")
write.table(DaumMovieReview,file="C:\\boxak\\Rstudy\\data\\daummovie.csv",sep=",",row.names = FALSE,
col.names = TRUE,fileEncoding = "CP949")
View(DaumMovieReview)
View(DaumMovieReview)
View(df2)
url <- "https://movie.daum.net/moviedb/grade?movieId=127122&type=netizen&page=1"
css.score <- page.getContent(url,css = "em.emph_grade")
css.review <- page.getContent(url,css = "p.desc_review")
source("function/crawler.R",encoding = "UTF-8")
getwd()
source("crawler.R",encoding = "UTF-8")
score <- vector()
review <- vector()
for(page in 1:20){
url <- paste0("https://movie.daum.net/moviedb/grade?movieId=127122&type=netizen&page=",page)
css.score <- page.getContent(url,css = "em.emph_grade")
css.review <- page.getContent(url,css = "p.desc_review")
if(length(css.score)==10 & length(css.review)==10){
score <- c(score,css.score)
review <- c(review,css.review)
}
}
DaumMovieReview2 <- data.frame(
댓글리뷰=review,
고객평점=score
)
View(DaumMovieReview2)
?write.csv
write.table(DaumMovieReview2,file="daummovie2.csv",sep=",",row.names = FALSE,
col.names = TRUE,fileEncoding = "CP949")
getwd()
url <- "http://media.daum.net/ranking/popular"
title <- page.getContent(url,css = "a.link_txt")
press <- page.getContent(url,css = "span.info_news")
press
title
title <- page.getContent(url,css = "div.cont_thumb > a.link_txt")
title
title <- page.getContent(url,css = "strong.tit_thumb > a.link_txt")
title
title <- page.getContent(url,css = "ul.list_news2 a.link_txt")
title
press
news50 <- data.frame(
newstitle = title,
newspapername = press
)
news50,file = "daumnews.csv",sep=",",row.names = FALSE,col.names = TRUE,
fileEncoding = "CP949")
write.table(news50,file = "daumnews.csv",sep=",",row.names = FALSE,col.names = TRUE,
fileEncoding = "CP949")
View(news50)
read_html
html_text
xml_text
# 한국일보 페이지(XML 패키지 사용)
install.packages("XML")
library(XML)
# httr 패키지 사용 - GET 방식 요청
install.packages("httr")
install.packages("httr")
library(httr)
library(rvest)
game = POST('http://www.gevolution.co.kr/score/gamescore.asp?t=3&m=0&d=week',
encode = 'form', body=list(txtPeriodW = '2020-03-15'))
View(L1)
title2 = html_nodes(read_html(game), 'a.tracktitle')
title2 = html_text(title2)
title2[1:10]
# 한국일보 페이지(XML 패키지 사용)
install.packages("XML")
library(XML)
imsi <- read_html("http://hankookilbo.com")
View(imsi)
t <- htmlParse(imsi)
t
content<- xpathSApply(t,"//p[@class='title']", xmlValue);
content
content <- gsub("[[:punct:][:cntrl:]]", "", content)
content
content <- trimws(content)
content
getwd()
setwd('function')
source("crawler.R",encoding = "UTF-8")
url <- "https://finance.naver.com/sise/sise_index_day.nhn?code=KPI200"
webpage <- read_html(url)
webpage
View(webpage)
url <- pasete0("https://finance.naver.com/sise/sise_index_day.nhn?code=KPI200&page=",1)
datelist <- page.getContent(url,css = "td.class")
url <- paste0("https://finance.naver.com/sise/sise_index_day.nhn?code=KPI200&page=",1)
datelist <- page.getContent(url,css = "td.class")
library(rvest)
datelist <- page.getContent(url,css = "td.class")
datelist <- page.getContent(url = url,css = "td.class",encoding = "UTF-8",trim = TRUE)
datelist <- page.getContent(url = url,css = "td.class",encoding = "CP949",trim = TRUE)
url
datelist
View(webpage)
webpage
datelist <- page.getContent(url = url,css = "td.date",encoding = "CP949",trim = TRUE)
datelist
pricelist <- page.getContent(url = url,css = "td.number_1",encoding = "CP949",trim = TRUE)
pricelist
pricelist <- page.getContent(url = url,css = "td:nth-child(2)",encoding = "CP949",trim = TRUE)
pricelist
pricelist <- page.getContent(url = url,css = "table.type_1 td:nth-child(2)",encoding = "CP949",trim = TRUE)
pricelist
encoding = "CP949"
endpage <- page.getAttr(url = url,css="td.pgRR > a",encoding = encoding)
endpage
View(endpage)
stringr::str_match("page=")
stringr::str_match(pattern = "page=")
stringr::str_match(endpage,pattern = "page=")
stringr::str_match(endpage,pattern = "page=[0-9]")
stringr::str_match(endpage,pattern = "page=^[0-9]")
stringr::str_match(endpage,pattern = "page=^0-9")
stringr::str_match(endpage,pattern = "^page=")
stringr::str_match(endpage,pattern = "page=^\\d")
stringr::str_match(endpage,pattern = "page=\\d")
stringr::str_match(endpage,pattern = "page=\\d*")
library(stringr)
endpage <- str_match(endpage,pattern = "page=\\d*")
endpage <- str_match(endpage,pattern = "\\d*")
endpage
endpage <- page.getAttr(url = url,css="td.pgRR > a",encoding = encoding)
endpage <- str_match(endpage,pattern = "page=\\d*")
endpage
endpage <- str_match(endpage,pattern = "\\d*")
endpage <- page.getAttr(url = url,css="td.pgRR > a",encoding = encoding)
endpage <- str_match(endpage,pattern = "page=\\d*")
endpage <- str_match(endpage,pattern = "[0-9]*")
endpage <- page.getAttr(url = url,css="td.pgRR > a",encoding = encoding)
endpage <- str_match(endpage,pattern = "page=\\d*")
endpage <- str_match(endpage,pattern = "^[0-9]*")
endpage <- page.getAttr(url = url,css="td.pgRR > a",encoding = encoding)
endpage <- str_match(endpage,pattern = "page=\\d*")
endpage <- str_remove(endpage,"[^0-9]")
endpage <- page.getAttr(url = url,css="td.pgRR > a",encoding = encoding)
endpage <- str_match(endpage,pattern = "page=\\d*")
endpage <- str_remove_all(endpage,"[^0-9]")
endpage <- as.integer(endpage)
url <- pasete0("https://finance.naver.com/sise/sise_index_day.nhn?code=KPI200&page=",587)
tempdate <- page.getContent(url = url,css = "td.date",encoding = "CP949",trim = TRUE)
tempprice <- page.getContent(url = url,css = "table.type_1 td:nth-child(2)",encoding = encoding,trim = TRUE)
tempdate
url <- pasete0("https://finance.naver.com/sise/sise_index_day.nhn?code=KPI200&page=",587)
url <- paset0("https://finance.naver.com/sise/sise_index_day.nhn?code=KPI200&page=",587)
tempdate <- page.getContent(url = url,css = "td.date",encoding = "CP949",trim = TRUE)
tempprice <- page.getContent(url = url,css = "table.type_1 td:nth-child(2)",encoding = encoding,trim = TRUE)
tempdate
url
url <- paste0("https://finance.naver.com/sise/sise_index_day.nhn?code=KPI200&page=",587)
tempdate <- page.getContent(url = url,css = "td.date",encoding = "CP949",trim = TRUE)
tempprice <- page.getContent(url = url,css = "table.type_1 td:nth-child(2)",encoding = encoding,trim = TRUE)
tempdate
tempprice
tempdate==""
tempdate[tempdate==""] <- NULL
tempdate[tempdate==""]
tempdate <- tempdate[tempdate!=""]
tempprice <- tempprice[tempprice!=""]
encoding = "CP949"
datelist <- vector()
pricelist <- vector()
endpage <- page.getAttr(url = url,css="td.pgRR > a",encoding = encoding)
endpage <- str_match(endpage,pattern = "page=\\d*")
endpage <- str_remove_all(endpage,"[^0-9]")
endpage <- as.integer(endpage)
for(page in 1:endpage){
url <- pasete0("https://finance.naver.com/sise/sise_index_day.nhn?code=KPI200&page=",page)
tempdate <- page.getContent(url = url,css = "td.date",encoding = "CP949",trim = TRUE)
tempprice <- page.getContent(url = url,css = "table.type_1 td:nth-child(2)",encoding = encoding,trim = TRUE)
tempdate <- tempdate[tempdate!=""]
tempprice <- tempprice[tempprice!=""]
datelist <- c(datelist,tempdate)
pricelist <- c(pricelist,tempprice)
}
endpage
endpage <- page.getAttr(url = url,css="td.pgRR > a",encoding = encoding)
View(endpage)
endpage <- str_match(endpage,pattern = "page=\\d*")
View(endpage)
endpage
encoding = "CP949"
datelist <- vector()
pricelist <- vector()
url <- paste0("https://finance.naver.com/sise/sise_index_day.nhn?code=KPI200&page=",1)
endpage <- page.getAttr(url = url,css="td.pgRR > a",encoding = encoding)
endpage <- str_match(endpage,pattern = "page=\\d*")
endpage <- str_remove_all(endpage,"[^0-9]")
endpage <- as.integer(endpage)
for(page in 1:endpage){
url <- paste0("https://finance.naver.com/sise/sise_index_day.nhn?code=KPI200&page=",page)
tempdate <- page.getContent(url = url,css = "td.date",encoding = "CP949",trim = TRUE)
tempprice <- page.getContent(url = url,css = "table.type_1 td:nth-child(2)",encoding = encoding,trim = TRUE)
tempdate <- tempdate[tempdate!=""]
tempprice <- tempprice[tempprice!=""]
datelist <- c(datelist,tempdate)
pricelist <- c(pricelist,tempprice)
}
lastday <- as.Date(datelist[1])
lastday <- as.Date(datelist[1],format="%Y.%m.%d")
firstday <- as.Date(datelist[length(datelist)],format="%Y.%m.%d")
seq(fistday:lastday,by="day")
seq(firstday:lastday,by="day")
seq(firstday,by="day",length.out = as.integer(lastday-firstday))
interval <- seq(firstday,by="day",length.out = as.integer(lastday-firstday))
levels(day2)
btype <- factor(
c("A", "O", "AB", "B", "O", "A"),
levels=c("A", "B", "O"))
btype
library(dplyr)
install.packages('dplyr')
install.packages('dplyr')
library(dplyr)
stockInfo <- cbind(datelist,pricelist)
head(stockInfo)
View(stockInfo)
colnames(interval)
colnames(interval) <- "datelist"
interval <- data.frame(datelist = interval)
interval
View(interval)
datelist <- as.Date(datelist,format="%Y.%m.%d")
stockInfo <- cbind(datelist,pricelist)
View(stockInfo)
View(stockInfo)
datelist
datelist <- as.Date(datelist,format="%Y.%m.%d")
pricelist
stockInfo <- cbind(datelist,pricelist)
View(stockInfo)
stockInfo[1,1]
lastday <- as.Date(datelist[1],format="%Y.%m.%d")
lastday
(2020-03-26)-(1970.01.01)
date(2020-03-26)-date(1970.01.01)
stockInfo
View(stockInfo)
for(page in 1:endpage){
url <- paste0("https://finance.naver.com/sise/sise_index_day.nhn?code=KPI200&page=",page)
tempdate <- page.getContent(url = url,css = "td.date",encoding = "CP949",trim = TRUE)
tempprice <- page.getContent(url = url,css = "table.type_1 td:nth-child(2)",encoding = encoding,trim = TRUE)
tempdate <- tempdate[tempdate!=""]
tempprice <- tempprice[tempprice!=""]
datelist <- c(datelist,tempdate)
pricelist <- c(pricelist,tempprice)
}
datelist <- vector()
pricelist <- vector()
url <- paste0("https://finance.naver.com/sise/sise_index_day.nhn?code=KPI200&page=",1)
endpage <- page.getAttr(url = url,css="td.pgRR > a",encoding = encoding)
endpage <- str_match(endpage,pattern = "page=\\d*")
endpage <- str_remove_all(endpage,"[^0-9]")
endpage <- as.integer(endpage)
for(page in 1:endpage){
url <- paste0("https://finance.naver.com/sise/sise_index_day.nhn?code=KPI200&page=",page)
tempdate <- page.getContent(url = url,css = "td.date",encoding = "CP949",trim = TRUE)
tempprice <- page.getContent(url = url,css = "table.type_1 td:nth-child(2)",encoding = encoding,trim = TRUE)
tempdate <- tempdate[tempdate!=""]
tempprice <- tempprice[tempprice!=""]
datelist <- c(datelist,tempdate)
pricelist <- c(pricelist,tempprice)
}
datelist
stockInfo <- cbind(datelist,pricelist)
View(stockInfo)
stockInfo$datelist <- as.Date(stockInfo$datelist,format="%Y.%m.%d")
head(stockInfo[datelist])
head(stockInfo["datelist"])
class(stockInfo)
stockInfo[1,1]
head(stockInfo[1])
head(stockInfo[1,])
head(stockInfo[,1])
stockInfo[,1] <- as.Date(stockInfo[,1],format="%Y.%m.%d")
stockInfo
